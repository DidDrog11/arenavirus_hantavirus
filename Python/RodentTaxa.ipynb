{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygbif\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "root = Path(\"/Users/ricardorivero/Documents/GitHub/arenavirus_hantavirus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in gzfile(file, \"rb\") : no se puede abrir la conexión\n",
      "Calls: extract_db -> readRDS -> gzfile\n",
      "Además: Warning message:\n",
      "In gzfile(file, \"rb\") :\n",
      "  cannot open compressed file '../../data/database/Project_ArHa_database_2025-08-20.rds', probable reason 'No such file or directory'\n",
      "Ejecución interrumpida\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['Rscript', '../R/RSD2CSV.R', '../../data/database/Project_ArHa_database_2025-08-20.rds', 'descriptive', '../../data/test_data/paper_data.csv']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m     cmd \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRscript\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         r_script,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         output_path\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[1;32m     10\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m call_extract_db(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../R/RSD2CSV.R\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/database/Project_ArHa_database_2025-08-20.rds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescriptive\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/test_data/paper_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mcall_extract_db\u001b[0;34m(r_script, input_path, db_name, output_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_extract_db\u001b[39m(r_script, input_path, db_name, output_path):\n\u001b[1;32m      3\u001b[0m     cmd \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRscript\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         r_script,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         output_path\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[0;32m---> 10\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['Rscript', '../R/RSD2CSV.R', '../../data/database/Project_ArHa_database_2025-08-20.rds', 'descriptive', '../../data/test_data/paper_data.csv']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def call_extract_db(r_script: str, input_rds: str, db_name: str, output_csv: str = None):\n",
    "    \"\"\"\n",
    "    Call the R extract_db script via subprocess.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r_script : str\n",
    "        Path to the R script (e.g., RSD2CSV.R).\n",
    "    input_rds : str\n",
    "        Path to the input .rds file.\n",
    "    db_name : str\n",
    "        Name of the database/table inside the .rds to extract.\n",
    "        Use '--list' to print available table names.\n",
    "    output_csv : str, optional\n",
    "        Path where the CSV will be written. Not required if db_name == '--list'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str or list\n",
    "        - If db_name == '--list': returns a list of available table names (strings).\n",
    "        - Otherwise: returns the output_csv path if extraction succeeded.\n",
    "    \"\"\"\n",
    "    print(\"Rscript at:\", shutil.which(\"Rscript\"))\n",
    "    \n",
    "    # Build command\n",
    "    if db_name == \"--list\":\n",
    "        cmd = [\"Rscript\", r_script, input_rds, \"--list\"]\n",
    "    else:\n",
    "        if output_csv is None:\n",
    "            raise ValueError(\"output_csv must be provided unless db_name == '--list'\")\n",
    "        cmd = [\"Rscript\", r_script, input_rds, db_name, output_csv]\n",
    "    \n",
    "    # Run Rscript\n",
    "    res = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    \n",
    "    print(\"Return code:\", res.returncode)\n",
    "    print(\"\\n--- STDOUT ---\\n\", res.stdout)\n",
    "    print(\"\\n--- STDERR ---\\n\", res.stderr)\n",
    "    \n",
    "    # Raises CalledProcessError if R returned non-zero\n",
    "    res.check_returncode()\n",
    "    \n",
    "    if db_name == \"--list\":\n",
    "        # Parse the table names from stdout\n",
    "        lines = [line.strip(\" -\") for line in res.stdout.splitlines() if line.strip()]\n",
    "        return lines\n",
    "    else:\n",
    "        return output_csv\n",
    "\n",
    "\n",
    "# Example usage: list mode\n",
    "# available = call_extract_db(\n",
    "#     \"arenavirus_hantavirus/R/RSD2CSV.R\",\n",
    "#     \"arenavirus_hantavirus/data/database/Project_ArHa_database_2025-08-20.rds\",\n",
    "#     \"--list\"\n",
    "# )\n",
    "# print(\"Available tables:\", available)\n",
    "\n",
    "# Example usage: extract mode\n",
    "# csv_path = call_extract_db(\n",
    "#     \"arenavirus_hantavirus/R/RSD2CSV.R\",\n",
    "#     \"arenavirus_hantavirus/data/database/Project_ArHa_database_2025-08-20.rds\",\n",
    "#     \"descriptives\",\n",
    "#     \"arenavirus_hantavirus/data/test_data/paper_data.csv\"\n",
    "# )\n",
    "# print(\"CSV written to:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote host_data.cleaned.csv and host_species_cleaning_map.csv\n"
     ]
    }
   ],
   "source": [
    "# taxonomy_clean_firstpass.py\n",
    "import re, time, unicodedata\n",
    "from typing import Optional, Dict, Any, List\n",
    "import pandas as pd\n",
    "from pygbif import species as gbif\n",
    "import requests_cache\n",
    "\n",
    "# --- Caching to be polite & fast ---\n",
    "requests_cache.install_cache(\"gbif_cache\", expire_after=7*24*3600)  # 7 days\n",
    "\n",
    "# --- Normalization helpers ---\n",
    "FLAG_TOKENS = {\"cf.\", \"aff.\", \"sp.\", \"spp.\", \"nr.\", \"gr.\", \"sensu\", \"sl.\", \"ss.\"}\n",
    "\n",
    "def normalize_name(name: str) -> Optional[str]:\n",
    "    \"\"\"Unicode-normalize, strip authorships/flags, collapse to Genus species.\"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return None\n",
    "    x = unicodedata.normalize(\"NFKC\", name).strip()\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    if x == \"\" or x.lower() in {\"na\", \"n/a\", \"unknown\", \"undetermined\"}:\n",
    "        return None\n",
    "    # Remove content in parentheses (often authorships)\n",
    "    x = re.sub(r\"\\([^)]*\\)\", \"\", x).strip()\n",
    "    # Remove terminal authorships and years (very rough but effective)\n",
    "    x = re.sub(r\"\\b[A-Z][a-zA-Z-]+(?:\\s*&\\s*[A-Z][a-zA-Z-]+)?(?:\\s*,\\s*\\d{4})?$\", \"\", x).strip()\n",
    "    # Token filter (drop from first flag onwards)\n",
    "    tokens = x.split(\" \")\n",
    "    cleaned = []\n",
    "    for t in tokens:\n",
    "        if t.lower() in FLAG_TOKENS:\n",
    "            break\n",
    "        cleaned.append(t)\n",
    "    if not cleaned:\n",
    "        return None\n",
    "    # Keep only Genus + species epithet (binomial) on first pass\n",
    "    if len(cleaned) >= 2:\n",
    "        genus = cleaned[0].capitalize()\n",
    "        species = cleaned[1].lower()\n",
    "        # Basic sanity: genus starts uppercase alpha\n",
    "        if not re.match(r\"^[A-Z][a-zA-Z-]+$\", genus):\n",
    "            return None\n",
    "        # species epithet can include hyphen but no capitals/digits\n",
    "        if not re.match(r\"^[a-z-]+$\", species):\n",
    "            return None\n",
    "        return f\"{genus} {species}\"\n",
    "    # If only a genus is present, return None on first pass (too ambiguous)\n",
    "    return None\n",
    "\n",
    "# --- GBIF resolution with gentle backoff ---\n",
    "def gbif_resolve(name: str, min_confidence: int = 85, retries: int = 3, sleep: float = 0.3) -> Dict[str, Any]:\n",
    "    \"\"\"Resolve via GBIF backbone; return rich dict with match metadata.\"\"\"\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            res = gbif.name_backbone(name=name, strict=False)\n",
    "            if not res or \"matchType\" not in res:\n",
    "                return {\"best_match\": None, \"confidence\": None, \"status\": \"no_match\", \"note\": \"empty_response\"}\n",
    "            # Consider a match acceptable if GBIF gives a canonical name + reasonable confidence\n",
    "            conf = res.get(\"confidence\", 0)\n",
    "            can = res.get(\"canonicalName\") or res.get(\"scientificName\")\n",
    "            status = res.get(\"status\")  # e.g., \"ACCEPTED\", \"SYNONYM\"\n",
    "            match_type = res.get(\"matchType\")\n",
    "            accepted_usage_key = res.get(\"acceptedUsageKey\") or res.get(\"usageKey\")\n",
    "            accepted_name = None\n",
    "            if status == \"SYNONYM\" and res.get(\"acceptedUsageKey\"):\n",
    "                # Look up the accepted name quickly (cached)\n",
    "                acc = gbif.name_usage(key=res[\"acceptedUsageKey\"])\n",
    "                accepted_name = acc.get(\"canonicalName\") or acc.get(\"scientificName\")\n",
    "            out = {\n",
    "                \"query\": name,\n",
    "                \"best_match\": can,\n",
    "                \"accepted_name\": accepted_name,\n",
    "                \"usageKey\": res.get(\"usageKey\"),\n",
    "                \"acceptedUsageKey\": accepted_usage_key,\n",
    "                \"rank\": res.get(\"rank\"),\n",
    "                \"status\": status,\n",
    "                \"matchType\": match_type,\n",
    "                \"confidence\": conf,\n",
    "                \"kingdom\": res.get(\"kingdom\"),\n",
    "                \"phylum\": res.get(\"phylum\"),\n",
    "                \"class\": res.get(\"class\"),\n",
    "                \"order\": res.get(\"order\"),\n",
    "                \"family\": res.get(\"family\"),\n",
    "                \"genus\": res.get(\"genus\"),\n",
    "                \"note\": None,\n",
    "                \"source\": \"GBIF\",\n",
    "            }\n",
    "            if conf is None or conf < min_confidence:\n",
    "                out[\"note\"] = f\"low_confidence({conf})\"\n",
    "                out[\"status\"] = \"low_confidence\"\n",
    "            return out\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            time.sleep(sleep * (i + 1))\n",
    "    return {\"query\": name, \"best_match\": None, \"confidence\": None, \"status\": \"error\", \"note\": last_err, \"source\": \"GBIF\"}\n",
    "\n",
    "def firstpass_cleaning(species_list: List[str],\n",
    "                       min_confidence: int = 85) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize and resolve names; return tidy mapping table.\n",
    "    Columns: original, normalized, cleaned_name, match_status, source, usageKey, acceptedUsageKey, confidence, note\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for raw in species_list:\n",
    "        normalized = normalize_name(raw)\n",
    "        if not normalized:\n",
    "            records.append({\n",
    "                \"original\": raw, \"normalized\": None, \"cleaned_name\": None,\n",
    "                \"match_status\": \"unusable_input\", \"source\": None,\n",
    "                \"usageKey\": None, \"acceptedUsageKey\": None, \"confidence\": None, \"note\": \"no_binomial\"\n",
    "            })\n",
    "            continue\n",
    "        m = gbif_resolve(normalized, min_confidence=min_confidence)\n",
    "        cleaned = None\n",
    "        status = m.get(\"status\")\n",
    "        if status in {\"ACCEPTED\", \"low_confidence\"} and m.get(\"best_match\"):\n",
    "            cleaned = m[\"best_match\"]\n",
    "        elif status == \"SYNONYM\" and (m.get(\"accepted_name\") or m.get(\"best_match\")):\n",
    "            cleaned = m.get(\"accepted_name\") or m.get(\"best_match\")\n",
    "        elif status == \"no_match\":\n",
    "            cleaned = normalized  # keep normalized binomial but flag as no_match\n",
    "        records.append({\n",
    "            \"original\": raw,\n",
    "            \"normalized\": normalized,\n",
    "            \"cleaned_name\": cleaned,\n",
    "            \"match_status\": status,\n",
    "            \"source\": m.get(\"source\"),\n",
    "            \"usageKey\": m.get(\"usageKey\"),\n",
    "            \"acceptedUsageKey\": m.get(\"acceptedUsageKey\"),\n",
    "            \"confidence\": m.get(\"confidence\"),\n",
    "            \"note\": m.get(\"note\"),\n",
    "        })\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example pipeline\n",
    "    host_data = host_data = pd.read_csv(root / \"data\" / \"test_data\" / \"host_data.csv\")\n",
    "    species_list = host_data[\"host_species\"].astype(str).unique().tolist()\n",
    "\n",
    "    mapping = firstpass_cleaning(species_list, min_confidence=85)\n",
    "\n",
    "    # Merge back into your dataset\n",
    "    host_data = host_data.merge(mapping[[\"original\",\"cleaned_name\",\"match_status\",\"confidence\"]],\n",
    "                                left_on=\"host_species\", right_on=\"original\", how=\"left\") \\\n",
    "                         .drop(columns=[\"original\"]) \\\n",
    "                         .rename(columns={\"cleaned_name\":\"species_clean\"})\n",
    "\n",
    "    host_data.to_csv(root / \"data\" / \"test_data\" / \"host_data.cleaned.csv\",\n",
    "                    index=False, encoding=\"utf-8\")\n",
    "\n",
    "    mapping.to_csv(root / \"data\" / \"test_data\" / \"host_species_cleaning_map.csv\",\n",
    "                index=False, encoding=\"utf-8\")\n",
    "    print(\"Wrote host_data.cleaned.csv and host_species_cleaning_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_rows': 55716, 'with_cleaned_match': 52028, 'corrected': 0, 'unchanged': 52028, 'missing_or_unmatched': 3688, 'correction_rate': 0.0}\n"
     ]
    }
   ],
   "source": [
    "#Quantify changes\n",
    "# Count mismatches between original and cleaned species names\n",
    "def quantify_corrections(host_data):\n",
    "    \"\"\"\n",
    "    Compare original host_species vs species_clean.\n",
    "    Returns dictionary with counts of corrected, unchanged, and missing values.\n",
    "    \"\"\"\n",
    "    total = len(host_data)\n",
    "    # Exclude rows where species_clean is NA (unmatched / unusable input)\n",
    "    valid = host_data.dropna(subset=[\"species_clean\"])\n",
    "    \n",
    "    corrected = (valid[\"host_species\"] != valid[\"species_clean\"]).sum()\n",
    "    unchanged = (valid[\"host_species\"] == valid[\"species_clean\"]).sum()\n",
    "    missing = host_data[\"species_clean\"].isna().sum()\n",
    "    \n",
    "    return {\n",
    "        \"total_rows\": total,\n",
    "        \"with_cleaned_match\": len(valid),\n",
    "        \"corrected\": corrected,\n",
    "        \"unchanged\": unchanged,\n",
    "        \"missing_or_unmatched\": missing,\n",
    "        \"correction_rate\": corrected / total if total > 0 else 0\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "\n",
    "host_data_cleaned = pd.read_csv(root / \"data\" / \"test_data\" / \"host_data.cleaned.csv\")\n",
    "\n",
    "summary = quantify_corrections(host_data_cleaned)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched species: 1\n",
      "Unmatched species: [nan]\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Get rows where the cleaning failed (species_clean is NaN)\n",
    "unmatched = host_data[host_data_cleaned[\"species_clean\"].isna()]\n",
    "\n",
    "# Show the unique original values that failed to resolve\n",
    "unmatched_species = unmatched[\"host_species\"].unique().tolist()\n",
    "\n",
    "print(\"Number of unmatched species:\", len(unmatched_species))\n",
    "print(\"Unmatched species:\", unmatched_species)\n",
    "unmatched_counts = unmatched[\"host_species\"].value_counts()\n",
    "print(unmatched_counts)\n",
    "\n",
    "unmatched"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
